{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally logistic regresion\n",
    "model=Sequential([\n",
    "    Input(shape=(1,)),\n",
    "    Dense(units=25,activation='relu'),\n",
    "    Dense(units=10,activation='relu'),\n",
    "    Dense(units=1,activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   1   1   4   4   7   8   8   9  11  11  13  14  14  15  16  17\n",
      "  18  19  20  20  21  21  22  24  26  27  27  28  32  32  34  34  35  35\n",
      "  36  36  36  37  38  38  38  40  40  40  41  42  45  46  46  46  47  48\n",
      "  50  52  52  52  53  53  57  57  57  57  58  61  62  64  68  68  69  71\n",
      "  73  74  75  75  79  80  80  85  85  87  89  89  90  91  91  93  95  95\n",
      "  95  98  98  98  98  98  98  98  99  99 102 102 103 103 105 105 106 108\n",
      " 111 112 112 112 114 115 116 116 117 119 119 121 121 122 123 124 125 126\n",
      " 127 128 129 130 130 130 131 132 133 134 134 138 142 142 143 143 143 143\n",
      " 144 146 147 148 149 150 150 151 152 153 154 156 157 159 159 159 160 160\n",
      " 160 161 162 162 164 166 166 167 167 169 170 171 172 172 173 175 176 177\n",
      " 179 180 181 183 185 186 187 188 189 189 189 189 189 190 190 191 191 191\n",
      " 193 194 195 195 197 200 200 200 201 202 202 203 205 206 207 212 214 214\n",
      " 216 216 216 217 217 219 219 219 219 222 222 224 224 224 225 226 226 228\n",
      " 229 229 230 230 230 232 232 232 232 233 234 236 240 241 241 242 243 245\n",
      " 246 246 250 251 251 252 252 252 252 252 253 253 253 255 255 257 258 258\n",
      " 262 263 264 267 267 268 269 270 270 271 272 273 276 276 276 276 278 279\n",
      " 279 282 282 283 284 284 287 288 288 293 295 297 300 301 301 303 304 304\n",
      " 308 313 313 315 315 317 320 322 322 324 325 326 329 330 330 335 337 337\n",
      " 337 338 339 339 340 341 341 341 343 343 345 345 345 345 347 347 347 348\n",
      " 348 349 353 356 357 358 360 364 364 366 366 368 368 369 372 372 372 372\n",
      " 373 374 375 376 378 379 379 383 384 384 385 385 385 386 386 387 387 389\n",
      " 389 391 391 392 392 392 392 393 395 395 396 396 397 397 397 399 400 400\n",
      " 401 401 402 402 403 406 406 407 407 408 410 412 413 414 414 414 415 416\n",
      " 417 418 419 419 421 421 426 427 431 434 435 437 438 440 441 441 443 444\n",
      " 444 446 449 450 452 452 452 454 454 455 455 455 456 458 459 459 460 460\n",
      " 461 461 463 466 468 469 470 470 471 471 472 472 472 473 473 474 474 475\n",
      " 475 476 476 476 480 483 484 484 487 488 489 489 491 491 492 492 495 496\n",
      " 496 496 498 498 499 500 501 502 502 503 504 504 505 505 505 507 508 509\n",
      " 509 510 510 510 510 511 511 512 512 515 515 516 517 518 520 521 524 524\n",
      " 524 525 530 531 537 539 540 544 546 546 546 547 550 550 553 555 555 555\n",
      " 555 556 557 559 560 561 562 562 563 563 563 564 564 565 565 565 565 566\n",
      " 568 569 569 571 571 573 573 574 574 576 577 579 580 586 587 592 593 595\n",
      " 600 600 601 603 604 606 606 607 607 608 608 609 610 611 611 612 612 612\n",
      " 612 614 616 619 619 623 624 624 625 625 627 627 632 633 633 633 635 637\n",
      " 637 637 638 639 639 639 640 640 641 642 645 646 647 647 647 648 651 652\n",
      " 652 653 654 655 655 655 656 656 657 658 658 658 658 658 659 659 661 662\n",
      " 662 663 663 663 663 664 667 669 669 671 671 671 672 674 674 675 675 675\n",
      " 676 680 681 681 681 681 683 683 683 684 684 685 686 687 688 689 690 690\n",
      " 691 691 692 694 696 697 697 698 698 698 699 699 700 701 702 703 703 705\n",
      " 708 709 709 710 713 717 719 719 723 724 725 726 726 726 727 728 728 729\n",
      " 729 731 731 732 733 733 734 734 735 735 735 738 738 739 740 742 744 744\n",
      " 747 748 748 749 749 750 750 751 751 753 753 757 758 758 760 761 761 763\n",
      " 763 763 763 766 766 768 768 769 770 771 772 773 775 775 775 775 776 778\n",
      " 779 779 782 782 783 784 786 786 789 790 791 791 791 792 794 794 795 795\n",
      " 795 797 797 797 797 799 800 801 801 803 804 805 806 810 812 814 814 815\n",
      " 815 815 815 817 818 818 819 819 821 821 822 823 823 824 825 826 827 828\n",
      " 828 828 829 829 830 831 832 833 834 835 836 836 837 838 838 839 840 841\n",
      " 842 848 850 851 851 853 853 854 856 856 857 857 860 860 860 862 863 863\n",
      " 866 866 866 868 868 870 871 871 871 871 871 872 875 875 876 877 878 878\n",
      " 879 880 880 882 882 882 883 883 884 884 885 885 890 891 891 894 895 895\n",
      " 895 896 896 897 897 899 902 902 905 906 907 907 908 911 912 913 914 915\n",
      " 919 919 920 920 921 921 922 924 927 928 928 928 928 928 929 931 931 933\n",
      " 934 934 935 935 942 942 942 945 945 947 950 951 952 952 952 953 954 954\n",
      " 955 955 956 956 957 957 957 957 957 957 958 958 959 960 961 962 962 967\n",
      " 967 969 970 971 971 972 972 973 974 975 977 980 981 982 985 986 987 987\n",
      " 989 991 992 992 995 995 996 996 998 998]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x=np.random.randint(0,1000,size=1000)\n",
    "x.sort()\n",
    "y=np.random.randint(0,2,size=1000)\n",
    "y.sort()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4317\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4017\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3491\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3465\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3476\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3488\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3419\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3235\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3538\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3787\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4021\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3788\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4064\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3557\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4655\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5081\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2852\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3128\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3571\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3225\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4172\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2779\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2723\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3068\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3428\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2958\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2927\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3497\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3083\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2464\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3243\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3543\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2508\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3435\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3212\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2873\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3349\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2483\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3618\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2983\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3437\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2536\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3462\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3286\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2782\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3040\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2297\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3664\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3004\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2754\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3313\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1948\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3207\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3063\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2584\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3084\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2242\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3233\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2288\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2888\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2538\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2004\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2861\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2464\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3484\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2125\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2437\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2917\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2001\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2971\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3941\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1663\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2522\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2329\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2101\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2605\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2160\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2337\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2478\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2866\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2739\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1779\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2155\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2282\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2724\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2235\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2546\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1970\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2183\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2154\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2252\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2319\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2371\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2384\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1722\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2145\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2187\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2100\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2093\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c0dbab8b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.9957746e-01],\n",
       "       [3.9661895e-11],\n",
       "       [3.9098867e-11]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([900,25,26])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ab jaise numerical calculation krta hai tu to jaise jab choti value hoti hai to float me krte Hi to decimal point me dikat hai to loss ko thoda sa aur accurate krne ke liye ham alag tarike se kam krte hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=Sequential([\n",
    "    Input(shape=(1,)),\n",
    "    Dense(units=25,activation='relu'),\n",
    "    Dense(units=10,activation='relu'),\n",
    "    Dense(units=1,activation='linear'),\n",
    "])\n",
    "m2.compile(loss=BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 45.7020\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5850\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7592\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7189\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7752\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6872\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7215\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6920\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7167\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6607\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6933\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6299\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7020\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6190\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6239\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5772\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6795\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5885\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5846\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6106\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6288\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6065\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5949\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6786\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6176\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5360\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6046\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5234\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5147\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5466\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5179\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4844\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4751\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5280\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4724\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4878\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4706\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4120\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4775\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4153\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5140\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4513\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3880\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4225\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4594\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3829\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4196\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4187\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3979\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3769\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3619\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3929\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3711\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3470\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3820\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3374\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4002\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2833\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3709\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3596\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3153\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3780\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2915\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3371\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3257\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3070\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2999\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3145\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2856\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3464\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2886\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2854\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3049\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2789\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2898\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2938\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2657\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3107\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2353\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2867\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2707\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2913\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2551\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2658\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2636\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2626\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2369\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2943\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2340\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2538\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2446\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2730\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2274\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2700\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2243\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2929\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2303\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2354\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c104a9090>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "tf.Tensor(\n",
      "[[9.9273479e-01]\n",
      " [7.3573482e-04]\n",
      " [7.3421974e-04]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Ab prediction krne ke liye hame alag function use krna pdega\n",
    "ot=m2.predict([900,25,26])\n",
    "true_ot=tf.nn.sigmoid(ot)\n",
    "print(true_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
